---
type: Project
title: Legalis
date: 2023-06-20
description: Machine Learning project about predicting the outcome of court cases in Germany.
images: ["/img/projects/legalis-thumbnail.png"]
focus: False
active: True
category: Machine Learning
tags: ["python", "academic", "sci-kit", "random forest", "BERT"]
link: https://ignitr.tech/legalis
github: https://github.com/LennardZuendorf/legalis
---

<Callout>
    - uni project around court case prediction with AI
    - prediction with Random Forest and BERT using German court case data
    - built with sci-kit learn (random forest) and a BERT model
</Callout>

## About the Project

Legalis was a University project for a machine learning and data science course. I wrote a paper at the University of Oslo about court case outcome prediction and this in the continuation of the project. The focus of this was rundimentary prediction of court case outcomes in German civil law cases.

I used bulk data from [openlegaldata](https:\openlegaldata.io), which included about 250.000 cases, out of which ca. 38k were usable for me. Based on this I trained a random forest classifier to predict the outcome of court cases. In the end I reached about a 60% accuracy, which is not great, but also not bad. Additionally I did some very basic training of a BERT model for a classification tasks, which also got about a 60% accuracy.

While this is a nice explorative project it has many flaws and definately doesn't prove any reliable prediction capabilities.

## Technology & Tools

<ProjectTech>
  <Link href="https://www.python.org/">
    <img
      src="https://img.shields.io/badge/python-3776AB?style=for-the-badge&logo=python&logoColor=white"
      alt="Python"
      className="rounded-sm"
    />
  </Link>
  <Link href="https://scikit-learn.org/stable/">
    <img
      src="https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white"
      alt="scikit-learn"
      className="rounded-sm"
    />
  </Link>
  <Link href="https://huggingface.co/docs/transformers/en/index">
    <img
        src="https://custom-icon-badges.demolab.com/badge/transformers-6B7280?style=for-the-badge&logoColor=black&logo=transformers"
        alt="Huggingface"
        className="rounded-sm"
    /> 
  </Link>
  <Link href="https://chat.openai.com">
    <img
      src="https://img.shields.io/badge/chatGPT-74aa9c?style=for-the-badge&logo=openai&logoColor=white"
      alt="Chat GPT"
      className="rounded-sm"
    />
  </Link>
</ProjectTech>

I very much enjoy all the features [ðŸ¤— huggingface](https://huggingface.co/) provides and heavily rely on it in my machine learning projects. Especially the hosting of Dataset, Model and Apps/Spaces.

For prediction, I have trained and optimized a random forest classifier aswell as a naive bayes, both using sci-kit and a BERT model for text classificaiton. For this I used transformers.

I used ChatGPT to extract the outcome as a binary label for 2800 cases and trained the models on that. It works great for the extraction of certain information from longer text (if you're willing to pay or have short texts).

## Future

I'm planning to update this to use a llama3 or mistral based german language model for classification, as the BERT performance was already pretty promising.
